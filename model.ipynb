{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance on using this dataset: https://www.kaggle.com/code/acelevin/identifying-playing-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from: https://www.kaggle.com/datasets/gunhcolab/object-detection-dataset-standard-52card-deck/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters:\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My attempt at a ResNet Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    ''' Res Block for an attempt at a ResNet architecture clone'''\n",
    "    def __init__(self, filters, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.filters = filters\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # downsample will halve the image size\n",
    "        strides = 2 if downsample else 1\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, 3, strides=strides, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        # Optional downsampling for the shortcut path\n",
    "        if downsample:\n",
    "            self.downsample_conv = tf.keras.layers.Conv2D(filters, 1, strides=2, padding='same', use_bias=False)\n",
    "            self.downsample_bn = tf.keras.layers.BatchNormalization()\n",
    "        else:\n",
    "            self.downsample_conv = None\n",
    "            \n",
    "    def call(self, inputs, training=False):\n",
    "        shortcut = inputs\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        if self.downsample_conv:\n",
    "            shortcut = self.downsample_conv(shortcut)\n",
    "            shortcut = self.downsample_bn(shortcut, training=training)\n",
    "\n",
    "        x = tf.keras.layers.add([x, shortcut])\n",
    "        return self.relu(x)\n",
    "        \n",
    "\n",
    "class MockResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MockResNet, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [\n",
    "            tf.keras.Input(shape=(300, 300, 3)),\n",
    "            #data_augmentation,\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\n",
    "            \n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(64),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            ResidualBlock(128, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(128),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "            ResidualBlock(256, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            # ResidualBlock(256),\n",
    "            # tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "            ResidualBlock(512, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            # ResidualBlock(512),\n",
    "            # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            #tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(52, activation='softmax')\n",
    "        ]\n",
    "        self.sequential = tf.keras.Sequential(self.architecture)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        return self.sequential(inputs, training=training)\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CardPredictor, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [        \n",
    "                tf.keras.layers.InputLayer((300, 300, 3)),\n",
    "                data_augmentation,\n",
    "                \n",
    "                tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.4),\n",
    "                \n",
    "                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                \n",
    "                tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(52, activation='softmax')        \n",
    "                ]\n",
    "        \n",
    "        self.sequential = tf.keras.Sequential(self.architecture, name=\"card_predictor\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Passes input image through the network. \"\"\"\n",
    "        return self.sequential(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "new_data = {}\n",
    "for key, inner_dict in data.items():\n",
    "    img_path = inner_dict['img_path']\n",
    "    value = inner_dict['class_label']\n",
    "    new_data[img_path] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((new_data.keys(), new_data.values()))\n",
    "\n",
    "def load_train_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(load_train_image, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:44:43.015215: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts in validation dataset:\n",
      "Label 0: 29, 106\n",
      "Label 1: 30, 107\n",
      "Label 2: 27, 108\n",
      "Label 3: 23, 103\n",
      "Label 4: 35, 105\n",
      "Label 5: 30, 113\n",
      "Label 6: 28, 111\n",
      "Label 7: 27, 110\n",
      "Label 8: 29, 109\n",
      "Label 9: 28, 110\n",
      "Label 10: 22, 113\n",
      "Label 11: 24, 114\n",
      "Label 12: 33, 94\n",
      "Label 13: 25, 115\n",
      "Label 14: 27, 111\n",
      "Label 15: 21, 113\n",
      "Label 16: 28, 103\n",
      "Label 17: 26, 114\n",
      "Label 18: 27, 107\n",
      "Label 19: 22, 102\n",
      "Label 20: 26, 95\n",
      "Label 21: 27, 113\n",
      "Label 22: 31, 110\n",
      "Label 23: 28, 101\n",
      "Label 24: 25, 108\n",
      "Label 25: 34, 101\n",
      "Label 26: 28, 101\n",
      "Label 27: 25, 101\n",
      "Label 28: 31, 105\n",
      "Label 29: 27, 110\n",
      "Label 30: 22, 108\n",
      "Label 31: 31, 113\n",
      "Label 32: 30, 108\n",
      "Label 33: 23, 110\n",
      "Label 34: 26, 109\n",
      "Label 35: 22, 109\n",
      "Label 36: 27, 108\n",
      "Label 37: 27, 105\n",
      "Label 38: 23, 106\n",
      "Label 39: 28, 109\n",
      "Label 40: 17, 108\n",
      "Label 41: 30, 112\n",
      "Label 42: 26, 110\n",
      "Label 43: 26, 108\n",
      "Label 44: 22, 109\n",
      "Label 45: 29, 102\n",
      "Label 46: 30, 99\n",
      "Label 47: 31, 109\n",
      "Label 48: 17, 110\n",
      "Label 49: 26, 107\n",
      "Label 50: 30, 106\n",
      "Label 51: 34, 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:44:50.076101: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# this cell it optional\n",
    "# this checks that the datasets  is balanced\n",
    "import collections\n",
    "v_counts = collections.Counter()\n",
    "t_counts = collections.Counter()\n",
    "\n",
    "\n",
    "for _, label in val_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    v_counts.update(label)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    t_counts.update(label)\n",
    "    \n",
    "print(\"Label counts in validation dataset:\")\n",
    "for i in range(52):\n",
    "    print(f\"Label {i}: {v_counts[i]}, {t_counts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CardPredictor()\n",
    "#model = MockResNet()\n",
    "#model = tf.keras.models.load_model(\"model_weights_89.keras\")\n",
    "model.compile(optimizer=model.optimizer, loss=model.loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 209ms/step - accuracy: 0.8509 - loss: 10751.3350 - val_accuracy: 0.8620 - val_loss: 9784.5518\n",
      "Epoch 2/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 214ms/step - accuracy: 0.8442 - loss: 13309.9189 - val_accuracy: 0.8815 - val_loss: 8711.1084\n",
      "Epoch 3/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 216ms/step - accuracy: 0.8650 - loss: 11565.9199 - val_accuracy: 0.8721 - val_loss: 11484.7246\n",
      "Epoch 4/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 214ms/step - accuracy: 0.8758 - loss: 10543.7910 - val_accuracy: 0.8699 - val_loss: 11041.3184\n",
      "Epoch 5/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 212ms/step - accuracy: 0.8615 - loss: 12944.3428 - val_accuracy: 0.8577 - val_loss: 16974.2871\n",
      "Epoch 6/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 211ms/step - accuracy: 0.8621 - loss: 15146.7959 - val_accuracy: 0.8671 - val_loss: 14851.2021\n",
      "Epoch 7/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 216ms/step - accuracy: 0.8485 - loss: 17245.9980 - val_accuracy: 0.8707 - val_loss: 14440.5352\n",
      "Epoch 8/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 216ms/step - accuracy: 0.8469 - loss: 21979.7031 - val_accuracy: 0.8591 - val_loss: 17048.4785\n",
      "Epoch 9/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 220ms/step - accuracy: 0.8601 - loss: 18450.2773 - val_accuracy: 0.8938 - val_loss: 12848.8525\n",
      "Epoch 10/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 219ms/step - accuracy: 0.8873 - loss: 15153.7520 - val_accuracy: 0.8642 - val_loss: 21326.8359\n",
      "Epoch 11/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 219ms/step - accuracy: 0.8847 - loss: 16124.0420 - val_accuracy: 0.8533 - val_loss: 29963.8242\n",
      "Epoch 12/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 219ms/step - accuracy: 0.8582 - loss: 25123.7988 - val_accuracy: 0.8844 - val_loss: 17332.0371\n",
      "Epoch 13/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 220ms/step - accuracy: 0.8668 - loss: 23852.2344 - val_accuracy: 0.8873 - val_loss: 20938.6855\n",
      "Epoch 14/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 222ms/step - accuracy: 0.8739 - loss: 23572.0000 - val_accuracy: 0.8656 - val_loss: 33700.8672\n",
      "Epoch 15/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 213ms/step - accuracy: 0.8688 - loss: 29103.1152 - val_accuracy: 0.8382 - val_loss: 43487.4375\n",
      "Epoch 16/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 235ms/step - accuracy: 0.8515 - loss: 36677.9062 - val_accuracy: 0.8721 - val_loss: 33162.0625\n",
      "Epoch 17/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 211ms/step - accuracy: 0.8771 - loss: 34279.4258 - val_accuracy: 0.8671 - val_loss: 34696.8477\n",
      "Epoch 18/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 205ms/step - accuracy: 0.8733 - loss: 33204.3086 - val_accuracy: 0.8764 - val_loss: 34440.1953\n",
      "Epoch 19/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 207ms/step - accuracy: 0.8818 - loss: 32094.1758 - val_accuracy: 0.8837 - val_loss: 30801.5430\n",
      "Epoch 20/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 207ms/step - accuracy: 0.8816 - loss: 34692.4453 - val_accuracy: 0.8880 - val_loss: 33488.4727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1335817f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
