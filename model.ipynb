{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance on using this dataset: https://www.kaggle.com/code/acelevin/identifying-playing-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from: https://www.kaggle.com/datasets/gunhcolab/object-detection-dataset-standard-52card-deck/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters:\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CardPredictor, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [        \n",
    "                tf.keras.layers.InputLayer((300, 300, 3)),\n",
    "                data_augmentation,\n",
    "                             \n",
    "                tf.keras.layers.Conv2D(32, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(64, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(128, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                \n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(256, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                \n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(512, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Flatten(),                                   # Flatten to vector\n",
    "                \n",
    "                tf.keras.layers.Dense(256, activation='relu'),               # Fully connected layer\n",
    "                tf.keras.layers.Dropout(0.5),                                # Prevent overfitting\n",
    "                tf.keras.layers.Dense(52, activation='softmax')\n",
    "                ]\n",
    "        \n",
    "        \n",
    "        self.sequential = tf.keras.Sequential(self.architecture, name=\"card_predictor\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Passes input image through the network. \"\"\"\n",
    "        return self.sequential(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "new_data = {}\n",
    "for key, inner_dict in data.items():\n",
    "    img_path = inner_dict['img_path']\n",
    "    value = inner_dict['class_label']\n",
    "    new_data[img_path] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((new_data.keys(), new_data.values()))\n",
    "\n",
    "def load_train_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(load_train_image, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 21:21:48.904347: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/5.png; No such file or directory\n",
      "2025-04-29 21:21:48.904360: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/0.png; No such file or directory\n",
      "2025-04-29 21:21:48.904369: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/2.png; No such file or directory\n",
      "2025-04-29 21:21:48.904378: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/1.png; No such file or directory\n",
      "2025-04-29 21:21:48.904442: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/9.png; No such file or directory\n",
      "2025-04-29 21:21:48.904456: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/3.png; No such file or directory\n",
      "2025-04-29 21:21:48.904461: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n",
      "\t [[{{node ReadFile}}]]\n",
      "2025-04-29 21:21:48.904468: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/7.png; No such file or directory\n",
      "2025-04-29 21:21:48.904479: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/6.png; No such file or directory\n",
      "2025-04-29 21:21:48.904491: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/4.png; No such file or directory\n",
      "2025-04-29 21:21:48.904509: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/8.png; No such file or directory\n",
      "2025-04-29 21:21:48.904534: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/10.png; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m v_counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[1;32m      5\u001b[0m t_counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n\u001b[1;32m      8\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist() \n\u001b[1;32m      9\u001b[0m     v_counts\u001b[38;5;241m.\u001b[39mupdate(label)\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# this cell it optional\n",
    "# this checks that the dataset is balanced\n",
    "import collections\n",
    "v_counts = collections.Counter()\n",
    "t_counts = collections.Counter()\n",
    "\n",
    "for _, label in val_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    v_counts.update(label)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    t_counts.update(label)\n",
    "    \n",
    "print(\"Label counts in validation dataset:\")\n",
    "for i in range(52):\n",
    "    print(f\"Label {i}: {v_counts[i]}, {t_counts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CardPredictor()\n",
    "model.build((None, 300, 300, 3))\n",
    "model.load_weights(\"model_weights_e80_a88.weights.h5\")\n",
    "\n",
    "model.compile(optimizer=model.optimizer, loss=model.loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 11/175\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 217ms/step - accuracy: 0.8960 - loss: 27802.2266"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('new_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture/batch_normalization/vars/0: (32,)\n",
      "architecture/batch_normalization/vars/1: (32,)\n",
      "architecture/batch_normalization/vars/2: (32,)\n",
      "architecture/batch_normalization/vars/3: (32,)\n",
      "architecture/batch_normalization_1/vars/0: (64,)\n",
      "architecture/batch_normalization_1/vars/1: (64,)\n",
      "architecture/batch_normalization_1/vars/2: (64,)\n",
      "architecture/batch_normalization_1/vars/3: (64,)\n",
      "architecture/batch_normalization_2/vars/0: (128,)\n",
      "architecture/batch_normalization_2/vars/1: (128,)\n",
      "architecture/batch_normalization_2/vars/2: (128,)\n",
      "architecture/batch_normalization_2/vars/3: (128,)\n",
      "architecture/batch_normalization_3/vars/0: (256,)\n",
      "architecture/batch_normalization_3/vars/1: (256,)\n",
      "architecture/batch_normalization_3/vars/2: (256,)\n",
      "architecture/batch_normalization_3/vars/3: (256,)\n",
      "architecture/batch_normalization_4/vars/0: (512,)\n",
      "architecture/batch_normalization_4/vars/1: (512,)\n",
      "architecture/batch_normalization_4/vars/2: (512,)\n",
      "architecture/batch_normalization_4/vars/3: (512,)\n",
      "architecture/conv2d/vars/0: (3, 3, 3, 32)\n",
      "architecture/conv2d/vars/1: (32,)\n",
      "architecture/conv2d_1/vars/0: (3, 3, 32, 64)\n",
      "architecture/conv2d_1/vars/1: (64,)\n",
      "architecture/conv2d_2/vars/0: (3, 3, 64, 128)\n",
      "architecture/conv2d_2/vars/1: (128,)\n",
      "architecture/conv2d_3/vars/0: (3, 3, 128, 256)\n",
      "architecture/conv2d_3/vars/1: (256,)\n",
      "architecture/conv2d_4/vars/0: (3, 3, 256, 512)\n",
      "architecture/conv2d_4/vars/1: (512,)\n",
      "architecture/dense/vars/0: (25088, 256)\n",
      "architecture/dense/vars/1: (256,)\n",
      "architecture/dense_1/vars/0: (256, 52)\n",
      "architecture/dense_1/vars/1: (52,)\n",
      "optimizer/vars/0: ()\n",
      "optimizer/vars/1: ()\n",
      "optimizer/vars/10: (3, 3, 32, 64)\n",
      "optimizer/vars/11: (3, 3, 32, 64)\n",
      "optimizer/vars/12: (64,)\n",
      "optimizer/vars/13: (64,)\n",
      "optimizer/vars/14: (64,)\n",
      "optimizer/vars/15: (64,)\n",
      "optimizer/vars/16: (64,)\n",
      "optimizer/vars/17: (64,)\n",
      "optimizer/vars/18: (3, 3, 64, 128)\n",
      "optimizer/vars/19: (3, 3, 64, 128)\n",
      "optimizer/vars/2: (3, 3, 3, 32)\n",
      "optimizer/vars/20: (128,)\n",
      "optimizer/vars/21: (128,)\n",
      "optimizer/vars/22: (128,)\n",
      "optimizer/vars/23: (128,)\n",
      "optimizer/vars/24: (128,)\n",
      "optimizer/vars/25: (128,)\n",
      "optimizer/vars/26: (3, 3, 128, 256)\n",
      "optimizer/vars/27: (3, 3, 128, 256)\n",
      "optimizer/vars/28: (256,)\n",
      "optimizer/vars/29: (256,)\n",
      "optimizer/vars/3: (3, 3, 3, 32)\n",
      "optimizer/vars/30: (256,)\n",
      "optimizer/vars/31: (256,)\n",
      "optimizer/vars/32: (256,)\n",
      "optimizer/vars/33: (256,)\n",
      "optimizer/vars/34: (3, 3, 256, 512)\n",
      "optimizer/vars/35: (3, 3, 256, 512)\n",
      "optimizer/vars/36: (512,)\n",
      "optimizer/vars/37: (512,)\n",
      "optimizer/vars/38: (512,)\n",
      "optimizer/vars/39: (512,)\n",
      "optimizer/vars/4: (32,)\n",
      "optimizer/vars/40: (512,)\n",
      "optimizer/vars/41: (512,)\n",
      "optimizer/vars/42: (25088, 256)\n",
      "optimizer/vars/43: (25088, 256)\n",
      "optimizer/vars/44: (256,)\n",
      "optimizer/vars/45: (256,)\n",
      "optimizer/vars/46: (256, 52)\n",
      "optimizer/vars/47: (256, 52)\n",
      "optimizer/vars/48: (52,)\n",
      "optimizer/vars/49: (52,)\n",
      "optimizer/vars/5: (32,)\n",
      "optimizer/vars/6: (32,)\n",
      "optimizer/vars/7: (32,)\n",
      "optimizer/vars/8: (32,)\n",
      "optimizer/vars/9: (32,)\n",
      "New weights:\n",
      "__________________________________:\n",
      "architecture/batch_normalization/vars/0: (32,)\n",
      "architecture/batch_normalization/vars/1: (32,)\n",
      "architecture/batch_normalization/vars/2: (32,)\n",
      "architecture/batch_normalization/vars/3: (32,)\n",
      "architecture/batch_normalization_1/vars/0: (32,)\n",
      "architecture/batch_normalization_1/vars/1: (32,)\n",
      "architecture/batch_normalization_1/vars/2: (32,)\n",
      "architecture/batch_normalization_1/vars/3: (32,)\n",
      "architecture/batch_normalization_2/vars/0: (64,)\n",
      "architecture/batch_normalization_2/vars/1: (64,)\n",
      "architecture/batch_normalization_2/vars/2: (64,)\n",
      "architecture/batch_normalization_2/vars/3: (64,)\n",
      "architecture/batch_normalization_3/vars/0: (64,)\n",
      "architecture/batch_normalization_3/vars/1: (64,)\n",
      "architecture/batch_normalization_3/vars/2: (64,)\n",
      "architecture/batch_normalization_3/vars/3: (64,)\n",
      "architecture/batch_normalization_4/vars/0: (128,)\n",
      "architecture/batch_normalization_4/vars/1: (128,)\n",
      "architecture/batch_normalization_4/vars/2: (128,)\n",
      "architecture/batch_normalization_4/vars/3: (128,)\n",
      "architecture/batch_normalization_5/vars/0: (128,)\n",
      "architecture/batch_normalization_5/vars/1: (128,)\n",
      "architecture/batch_normalization_5/vars/2: (128,)\n",
      "architecture/batch_normalization_5/vars/3: (128,)\n",
      "architecture/conv2d/vars/0: (3, 3, 3, 32)\n",
      "architecture/conv2d_1/vars/0: (3, 3, 32, 32)\n",
      "architecture/conv2d_2/vars/0: (3, 3, 32, 64)\n",
      "architecture/conv2d_3/vars/0: (3, 3, 64, 64)\n",
      "architecture/conv2d_4/vars/0: (3, 3, 64, 128)\n",
      "architecture/conv2d_5/vars/0: (3, 3, 128, 128)\n",
      "architecture/dense/vars/0: (128, 256)\n",
      "architecture/dense/vars/1: (256,)\n",
      "architecture/dense_1/vars/0: (256, 52)\n",
      "architecture/dense_1/vars/1: (52,)\n",
      "optimizer/vars/0: ()\n",
      "optimizer/vars/1: ()\n",
      "optimizer/vars/10: (32,)\n",
      "optimizer/vars/11: (32,)\n",
      "optimizer/vars/12: (32,)\n",
      "optimizer/vars/13: (32,)\n",
      "optimizer/vars/14: (3, 3, 32, 64)\n",
      "optimizer/vars/15: (3, 3, 32, 64)\n",
      "optimizer/vars/16: (64,)\n",
      "optimizer/vars/17: (64,)\n",
      "optimizer/vars/18: (64,)\n",
      "optimizer/vars/19: (64,)\n",
      "optimizer/vars/2: (3, 3, 3, 32)\n",
      "optimizer/vars/20: (3, 3, 64, 64)\n",
      "optimizer/vars/21: (3, 3, 64, 64)\n",
      "optimizer/vars/22: (64,)\n",
      "optimizer/vars/23: (64,)\n",
      "optimizer/vars/24: (64,)\n",
      "optimizer/vars/25: (64,)\n",
      "optimizer/vars/26: (3, 3, 64, 128)\n",
      "optimizer/vars/27: (3, 3, 64, 128)\n",
      "optimizer/vars/28: (128,)\n",
      "optimizer/vars/29: (128,)\n",
      "optimizer/vars/3: (3, 3, 3, 32)\n",
      "optimizer/vars/30: (128,)\n",
      "optimizer/vars/31: (128,)\n",
      "optimizer/vars/32: (3, 3, 128, 128)\n",
      "optimizer/vars/33: (3, 3, 128, 128)\n",
      "optimizer/vars/34: (128,)\n",
      "optimizer/vars/35: (128,)\n",
      "optimizer/vars/36: (128,)\n",
      "optimizer/vars/37: (128,)\n",
      "optimizer/vars/38: (128, 256)\n",
      "optimizer/vars/39: (128, 256)\n",
      "optimizer/vars/4: (32,)\n",
      "optimizer/vars/40: (256,)\n",
      "optimizer/vars/41: (256,)\n",
      "optimizer/vars/42: (256, 52)\n",
      "optimizer/vars/43: (256, 52)\n",
      "optimizer/vars/44: (52,)\n",
      "optimizer/vars/45: (52,)\n",
      "optimizer/vars/5: (32,)\n",
      "optimizer/vars/6: (32,)\n",
      "optimizer/vars/7: (32,)\n",
      "optimizer/vars/8: (3, 3, 32, 32)\n",
      "optimizer/vars/9: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('model_weights_e40_a82.weights.h5', 'r') as f:\n",
    "    def recursively_print(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"{name}: {obj.shape}\")\n",
    "    \n",
    "    f.visititems(recursively_print)\n",
    "    \n",
    "    \n",
    "print(\"New weights:\")\n",
    "print(\"__________________________________:\")\n",
    "\n",
    "with h5py.File('new_weights.weights.h5', 'r') as f:\n",
    "    def recursively_print(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"{name}: {obj.shape}\")\n",
    "    \n",
    "    f.visititems(recursively_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video capture and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RioVK/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = CardPredictor()\n",
    "model.build((None, 300, 300, 3))\n",
    "model.load_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 21:34:38.881 python[10625:18927309] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'c' to capture and classify a card.\n",
      "Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 21:34:42.497 python[10625:18927309] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-29 21:34:42.497 python[10625:18927309] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Exiting.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = CardPredictor()\n",
    "model.build((None, 300, 300, 3))\n",
    "model.load_weights('model_weights.weights.h5')\n",
    "\n",
    "values = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "suits = ['spade', 'heart', 'diamond', 'club']\n",
    "class_names = [f\"{v} of {s}\" for v in values for s in suits]\n",
    "# === Start camera ===\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "if not cam.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'c' to capture and classify a card.\")\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Show the camera feed\n",
    "    cv2.imshow(\"Live Feed - Press 'c' to classify, 'q' to quit\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        print(\"Capturing frame...\")\n",
    "\n",
    "        # Preprocess the frame\n",
    "        img = cv2.resize(frame, (300, 300))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        pred = model(img, training=False)\n",
    "        predicted_class = tf.argmax(pred, axis=1).numpy()[0]\n",
    "        # check dataset to fix the indexing here.\n",
    "        class_name = class_names[predicted_class]\n",
    "\n",
    "        # Overlay prediction and display result on the top right corner of the image. \n",
    "        display_frame = frame.copy()\n",
    "        cv2.putText(display_frame, f\"Prediction: {class_name}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Prediction\", display_frame)\n",
    "        cv2.waitKey(1500)  # Show prediction for 1.5 seconds\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
