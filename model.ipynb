{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance on using this dataset: https://www.kaggle.com/code/acelevin/identifying-playing-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from: https://www.kaggle.com/datasets/gunhcolab/object-detection-dataset-standard-52card-deck/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters:\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CardPredictor, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [        \n",
    "                tf.keras.layers.InputLayer((300, 300, 3)),\n",
    "                data_augmentation,\n",
    "                \n",
    "                tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                \n",
    "                tf.keras.layers.MaxPooling2D(2, 2),\n",
    "                tf.keras.layers.Dropout(0.4),\n",
    "                \n",
    "                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                \n",
    "                tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(52, activation='softmax')        \n",
    "                ]\n",
    "        \n",
    "        self.sequential = tf.keras.Sequential(self.architecture, name=\"card_predictor\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Passes input image through the network. \"\"\"\n",
    "        return self.sequential(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "new_data = {}\n",
    "for key, inner_dict in data.items():\n",
    "    img_path = inner_dict['img_path']\n",
    "    value = inner_dict['class_label']\n",
    "    new_data[img_path] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:59:27.620325: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-29 20:59:27.620351: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-04-29 20:59:27.620357: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-04-29 20:59:27.620377: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-29 20:59:27.620399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((new_data.keys(), new_data.values()))\n",
    "\n",
    "def load_train_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(load_train_image, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:59:32.880452: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/6.png; No such file or directory\n",
      "2025-04-29 20:59:32.880471: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/4.png; No such file or directory\n",
      "2025-04-29 20:59:32.880482: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/3.png; No such file or directory\n",
      "2025-04-29 20:59:32.880492: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/1.png; No such file or directory\n",
      "2025-04-29 20:59:32.880500: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/5.png; No such file or directory\n",
      "2025-04-29 20:59:32.880512: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/0.png; No such file or directory\n",
      "2025-04-29 20:59:32.880517: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/2.png; No such file or directory\n",
      "2025-04-29 20:59:32.880537: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/7.png; No such file or directory\n",
      "2025-04-29 20:59:32.880560: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n",
      "\t [[{{node ReadFile}}]]\n",
      "2025-04-29 20:59:32.880632: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/8.png; No such file or directory\n",
      "2025-04-29 20:59:32.880639: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/9.png; No such file or directory\n",
      "2025-04-29 20:59:32.880646: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/10.png; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m v_counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[1;32m      5\u001b[0m t_counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n\u001b[1;32m      8\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist() \n\u001b[1;32m      9\u001b[0m     v_counts\u001b[38;5;241m.\u001b[39mupdate(label)\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteSkip::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# this cell it optional\n",
    "# this checks that the dataset is balanced\n",
    "import collections\n",
    "v_counts = collections.Counter()\n",
    "t_counts = collections.Counter()\n",
    "\n",
    "for _, label in val_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    v_counts.update(label)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    t_counts.update(label)\n",
    "    \n",
    "print(\"Label counts in validation dataset:\")\n",
    "for i in range(52):\n",
    "    print(f\"Label {i}: {v_counts[i]}, {t_counts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CardPredictor()\n",
    "#model = tf.keras.models.load_model(\"MODEL_PATH\")\n",
    "\n",
    "model.compile(optimizer=model.optimizer, loss=model.loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:59:39.694162: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-04-29 20:59:40.273628: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/3.png; No such file or directory\n",
      "2025-04-29 20:59:40.273642: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/2.png; No such file or directory\n",
      "2025-04-29 20:59:40.273648: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/1.png; No such file or directory\n",
      "2025-04-29 20:59:40.273691: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/6.png; No such file or directory\n",
      "2025-04-29 20:59:40.273710: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/7.png; No such file or directory\n",
      "2025-04-29 20:59:40.273727: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/5.png; No such file or directory\n",
      "2025-04-29 20:59:40.273737: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/0.png; No such file or directory\n",
      "2025-04-29 20:59:40.273756: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/4.png; No such file or directory\n",
      "2025-04-29 20:59:40.273801: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/8.png; No such file or directory\n",
      "2025-04-29 20:59:40.273859: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n",
      "\t [[{{node ReadFile}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "2025-04-29 20:59:40.273867: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16135503077720932594\n",
      "2025-04-29 20:59:40.273870: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16043105096319967408\n",
      "2025-04-29 20:59:40.273875: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 7972759302097665388\n",
      "2025-04-29 20:59:40.273879: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3631202541226351436\n",
      "2025-04-29 20:59:40.273884: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12876051680499329582\n",
      "2025-04-29 20:59:40.273891: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17721818461633636884\n",
      "2025-04-29 20:59:40.273895: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/9.png; No such file or directory\n",
      "2025-04-29 20:59:40.273911: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15373394189232712326\n",
      "2025-04-29 20:59:40.273938: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: imgs/single/10.png; No such file or directory\n",
      "2025-04-29 20:59:40.273956: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 4903681774219456738\n",
      "2025-04-29 20:59:40.273979: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17932353271288221822\n",
      "2025-04-29 20:59:40.273985: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 4088310396830847538\n",
      "2025-04-29 20:59:40.273987: I tensorflow/core/framework/local_rendezvous.cc:427] Local rendezvous send item cancelled. Key hash: 11250097013027123270\n",
      "2025-04-29 20:59:40.273991: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 7803159753340958228\n",
      "2025-04-29 20:59:40.273994: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11322422384482056136\n",
      "2025-04-29 20:59:40.273998: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1856597555687692502\n",
      "2025-04-29 20:59:40.274001: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 10267907977668548886\n",
      "2025-04-29 20:59:40.274006: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9364961166178048526\n",
      "2025-04-29 20:59:40.274009: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9258338551237188498\n",
      "2025-04-29 20:59:40.274012: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3599197680298628684\n",
      "2025-04-29 20:59:40.274016: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9489432430160772564\n",
      "2025-04-29 20:59:40.274020: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9058685797823995354\n",
      "2025-04-29 20:59:40.274024: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3058373426318319282\n",
      "2025-04-29 20:59:40.274028: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 499161355480654093\n",
      "2025-04-29 20:59:40.274031: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17607412589646725657\n",
      "2025-04-29 20:59:40.274036: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 8569913586784355145\n",
      "2025-04-29 20:59:40.274039: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9230857787116372353\n",
      "2025-04-29 20:59:40.274042: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 2555918831619646759\n",
      "2025-04-29 20:59:40.274047: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17402736770312365719\n",
      "2025-04-29 20:59:40.274050: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1916708988720270317\n",
      "2025-04-29 20:59:40.274054: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3717869373221653261\n",
      "2025-04-29 20:59:40.274058: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6226613352452271609\n",
      "2025-04-29 20:59:40.274062: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5281988526789677905\n",
      "2025-04-29 20:59:40.274067: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9079271528280748935\n",
      "2025-04-29 20:59:40.274072: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17972175377401444349\n",
      "2025-04-29 20:59:40.274076: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13163776068255087071\n",
      "2025-04-29 20:59:40.274080: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 7303493675195046477\n",
      "2025-04-29 20:59:40.274083: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16986374466607840001\n",
      "2025-04-29 20:59:40.274091: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 15600672394199005469\n",
      "2025-04-29 20:59:40.274095: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3204147997582458027\n",
      "2025-04-29 20:59:40.274099: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 130116456226979675\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/adam/Add_28/_84]]\n  (1) NOT_FOUND:  Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_18878]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/csci1430/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/adam/Add_28/_84]]\n  (1) NOT_FOUND:  Error in user-defined function passed to ParallelMapDatasetV2:1 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::Shuffle::ParallelMapV2: imgs/single/0.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_18878]"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video capture and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jad/miniforge3/envs/csci1430/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = CardPredictor()\n",
    "model.build((None, 300, 300, 3))\n",
    "model.load_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'c' to capture and classify a card.\n",
      "Press 'q' to quit.\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Capturing frame...\n",
      "Exiting.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class CardPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CardPredictor, self).__init__()\n",
    "        self.architecture = [        \n",
    "            tf.keras.layers.InputLayer((300, 300, 3)),\n",
    "            tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(52, activation='softmax')        \n",
    "        ]\n",
    "        self.sequential = tf.keras.Sequential(self.architecture)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "# Load model\n",
    "model = CardPredictor()\n",
    "model.build((None, 300, 300, 3))\n",
    "model.load_weights('model_weights.weights.h5')\n",
    "\n",
    "class_names = [f'{v}{s}' for v in '23456789TJQKA' for s in 'CDHS'] # this returns what the card is from the number returned by the model, might be wrong, check labels to make sure it is correct, or use the dictionary instead.  \n",
    "\n",
    "# === Start camera ===\n",
    "cam = cv2.VideoCapture(2)\n",
    "\n",
    "if not cam.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'c' to capture and classify a card.\")\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Show the camera feed\n",
    "    cv2.imshow(\"Live Feed - Press 'c' to classify, 'q' to quit\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        print(\"Capturing frame...\")\n",
    "\n",
    "        # Preprocess the frame\n",
    "        img = cv2.resize(frame, (300, 300))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        pred = model(img, training=False)\n",
    "        predicted_class = tf.argmax(pred, axis=1).numpy()[0]\n",
    "        # check dataset to fix the indexing here.\n",
    "        class_name = class_names[predicted_class]\n",
    "\n",
    "        # Overlay prediction and display result on the top right corner of the image. \n",
    "        display_frame = frame.copy()\n",
    "        cv2.putText(display_frame, f\"Prediction: {class_name}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Prediction\", display_frame)\n",
    "        cv2.waitKey(1500)  # Show prediction for 1.5 seconds\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
