{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance on using this dataset: https://www.kaggle.com/code/acelevin/identifying-playing-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from: https://www.kaggle.com/datasets/gunhcolab/object-detection-dataset-standard-52card-deck/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters:\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My attempt at a ResNet Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    ''' Res Block for an attempt at a ResNet architecture clone'''\n",
    "    def __init__(self, filters, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.filters = filters\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # downsample will halve the image size\n",
    "        strides = 2 if downsample else 1\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, 3, strides=strides, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        # Optional downsampling for the shortcut path\n",
    "        if downsample:\n",
    "            self.downsample_conv = tf.keras.layers.Conv2D(filters, 1, strides=2, padding='same', use_bias=False)\n",
    "            self.downsample_bn = tf.keras.layers.BatchNormalization()\n",
    "        else:\n",
    "            self.downsample_conv = None\n",
    "            \n",
    "    def call(self, inputs, training=False):\n",
    "        shortcut = inputs\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        if self.downsample_conv:\n",
    "            shortcut = self.downsample_conv(shortcut)\n",
    "            shortcut = self.downsample_bn(shortcut, training=training)\n",
    "\n",
    "        x = tf.keras.layers.add([x, shortcut])\n",
    "        return self.relu(x)\n",
    "        \n",
    "\n",
    "class MockResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MockResNet, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [\n",
    "            tf.keras.Input(shape=(300, 300, 3)),\n",
    "            #data_augmentation,\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\n",
    "            \n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(64),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            ResidualBlock(128, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            ResidualBlock(128),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "            ResidualBlock(256, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            # ResidualBlock(256),\n",
    "            # tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "            ResidualBlock(512, downsample=True),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            # ResidualBlock(512),\n",
    "            # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            #tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(52, activation='softmax')\n",
    "        ]\n",
    "        self.sequential = tf.keras.Sequential(self.architecture)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        return self.sequential(inputs, training=training)\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CardPredictor, self).__init__()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        \n",
    "        data_augmentation = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.RandomRotation(0.1),\n",
    "                                tf.keras.layers.RandomZoom(0.1),\n",
    "                            ])\n",
    "        \n",
    "        self.architecture = [        \n",
    "                tf.keras.layers.InputLayer((300, 300, 3)),\n",
    "                data_augmentation,\n",
    "                             \n",
    "                tf.keras.layers.Conv2D(32, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(64, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(128, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                \n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(256, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                \n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Conv2D(512, (3, 3)),       # Conv + ReLU\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                tf.keras.layers.Flatten(),                                   # Flatten to vector\n",
    "                \n",
    "                tf.keras.layers.Dense(256, activation='relu'),               # Fully connected layer\n",
    "                tf.keras.layers.Dropout(0.5),                                # Prevent overfitting\n",
    "                tf.keras.layers.Dense(52, activation='softmax')\n",
    "                ]\n",
    "        \n",
    "        self.sequential = tf.keras.Sequential(self.architecture, name=\"card_predictor\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Passes input image through the network. \"\"\"\n",
    "        return self.sequential(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(labels, predictions): \n",
    "           \"\"\" Loss function for the model. \"\"\"\n",
    "           return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "new_data = {}\n",
    "for key, inner_dict in data.items():\n",
    "    img_path = inner_dict['img_path']\n",
    "    value = inner_dict['class_label']\n",
    "    new_data[img_path] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((new_data.keys(), new_data.values()))\n",
    "\n",
    "def load_train_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(load_train_image, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts in validation dataset:\n",
      "Label 0: 29, 104\n",
      "Label 1: 22, 110\n",
      "Label 2: 35, 108\n",
      "Label 3: 27, 112\n",
      "Label 4: 35, 101\n",
      "Label 5: 27, 108\n",
      "Label 6: 28, 113\n",
      "Label 7: 25, 104\n",
      "Label 8: 29, 109\n",
      "Label 9: 28, 109\n",
      "Label 10: 34, 109\n",
      "Label 11: 23, 94\n",
      "Label 12: 28, 110\n",
      "Label 13: 27, 114\n",
      "Label 14: 18, 113\n",
      "Label 15: 25, 109\n",
      "Label 16: 24, 109\n",
      "Label 17: 21, 98\n",
      "Label 18: 23, 111\n",
      "Label 19: 22, 102\n",
      "Label 20: 33, 110\n",
      "Label 21: 24, 101\n",
      "Label 22: 26, 104\n",
      "Label 23: 33, 111\n",
      "Label 24: 38, 99\n",
      "Label 25: 21, 107\n",
      "Label 26: 26, 101\n",
      "Label 27: 21, 105\n",
      "Label 28: 9, 112\n",
      "Label 29: 29, 105\n",
      "Label 30: 18, 117\n",
      "Label 31: 25, 108\n",
      "Label 32: 28, 105\n",
      "Label 33: 30, 112\n",
      "Label 34: 26, 120\n",
      "Label 35: 20, 111\n",
      "Label 36: 22, 110\n",
      "Label 37: 29, 109\n",
      "Label 38: 22, 102\n",
      "Label 39: 18, 114\n",
      "Label 40: 29, 100\n",
      "Label 41: 28, 107\n",
      "Label 42: 26, 114\n",
      "Label 43: 34, 113\n",
      "Label 44: 25, 104\n",
      "Label 45: 38, 100\n",
      "Label 46: 26, 109\n",
      "Label 47: 31, 109\n",
      "Label 48: 32, 108\n",
      "Label 49: 24, 108\n",
      "Label 50: 28, 108\n",
      "Label 51: 35, 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:29:25.803133: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# this cell it optional\n",
    "# this checks that the datasets  is balanced\n",
    "import collections\n",
    "v_counts = collections.Counter()\n",
    "t_counts = collections.Counter()\n",
    "\n",
    "\n",
    "for _, label in val_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    v_counts.update(label)\n",
    "\n",
    "for _, label in train_dataset:\n",
    "    label = label.numpy().tolist() \n",
    "    t_counts.update(label)\n",
    "    \n",
    "print(\"Label counts in validation dataset:\")\n",
    "for i in range(52):\n",
    "    print(f\"Label {i}: {v_counts[i]}, {t_counts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CardPredictor()\n",
    "model = MockResNet()\n",
    "#model = tf.keras.models.load_model(\"model_weights_89.keras\")\n",
    "model.compile(optimizer=model.optimizer, loss=model.loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](mock_res_net_8_1/sequential_13_1/dense_16_1/Softmax, ArgMax, in_top_k/InTopKV2/k)' with input shapes: [?,52], [], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/cs1430/lib/python3.9/site-packages/keras/src/backend/tensorflow/math.py:47\u001b[0m, in \u001b[0;36min_top_k\u001b[0;34m(targets, predictions, k)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21min_top_k\u001b[39m(targets, predictions, k):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](mock_res_net_8_1/sequential_13_1/dense_16_1/Softmax, ArgMax, in_top_k/InTopKV2/k)' with input shapes: [?,52], [], []."
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_weights_89.keras')  # can also use .h5 extension if preferred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
